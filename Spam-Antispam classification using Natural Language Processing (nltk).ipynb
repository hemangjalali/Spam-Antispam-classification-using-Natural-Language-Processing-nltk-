{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam-Antispam classification using Natural Language Processing (nltk)\n",
    "#  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Hemang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hemang\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import relevant Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   v1                                                 v2\n",
       "0   0  Go until jurong point, crazy.. Available only ...\n",
       "1   0                      Ok lar... Joking wif u oni...\n",
       "2   1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   0  U dun say so early hor... U c already then say...\n",
       "4   0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('spam.csv',encoding = 'ANSI') # Read Dataset and store as a DataFrame using Pandas\n",
    "df = df[['v1','v2']] # Keep only necessary columns\n",
    "df['v1']=df['v1'].map({'ham':0,'spam':1}) # Map ham as '0' and Spam as '1'\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns = {'v1': 'spam', 'v2': 'text'}, inplace = True) # Re-naming columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>go until jurong point, crazy.. available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ok lar... joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>free entry in 2 a wkly comp to win fa cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>u dun say so early hor... u c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>nah i don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   spam                                               text\n",
       "0     0  go until jurong point, crazy.. available only ...\n",
       "1     0                      ok lar... joking wif u oni...\n",
       "2     1  free entry in 2 a wkly comp to win fa cup fina...\n",
       "3     0  u dun say so early hor... u c already then say...\n",
       "4     0  nah i don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'] = df['text'].str.lower() # converting text to lowercase\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downloading stopwords package from nltk module\n",
    "# Creating a list of stop words and punctuations to omit from the 2 lists created above using \n",
    "# bag_of_words_features_filtered() function defined in next cell\n",
    "\n",
    "nltk.corpus.stopwords.words(\"english\")  \n",
    "useless_words = nltk.corpus.stopwords.words(\"english\") + list(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_features_filtered(words):  # Function which filters words that have some significance \n",
    "    my_dict = dict( [ (word, True) for word in words if not word in useless_words] )\n",
    "    return my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the 'text' column from the Dataframe df having column spam = '1' \n",
    "\n",
    "dfspam = df[df['spam']==1]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the 'text' column from the Dataframe df having column spam = '1' \n",
    "\n",
    "dfham = df[df['spam']==0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'mobil': True,\n",
       "   'month': True,\n",
       "   'entitl': True,\n",
       "   'updat': True,\n",
       "   'latest': True,\n",
       "   'colour': True,\n",
       "   'camera': True,\n",
       "   'free': True,\n",
       "   'call': True,\n",
       "   '08002986030': True},\n",
       "  'spam'),\n",
       " ({'six': True,\n",
       "   'chanc': True,\n",
       "   'win': True,\n",
       "   'cash': True,\n",
       "   '100': True,\n",
       "   '20,000': True,\n",
       "   'pound': True,\n",
       "   'txt': True,\n",
       "   'csh11': True,\n",
       "   'send': True,\n",
       "   '87575.': True,\n",
       "   'cost': True,\n",
       "   '150p/day': True,\n",
       "   '6day': True,\n",
       "   '16+': True,\n",
       "   'tsandc': True,\n",
       "   'appli': True,\n",
       "   'repli': True,\n",
       "   'info': True},\n",
       "  'spam')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of spam emails after data cleaning, word segregation performed on each email one-by-one\n",
    "\n",
    "spam_list=[]\n",
    "stemmer = PorterStemmer()\n",
    "for i in range(dfspam.shape[0]):\n",
    "    word = word_tokenize(dfspam.iloc[i])  #Tokenizing\n",
    "    word1 = [word for word in word if len(word)>2] #  Removing single char words\n",
    "    words = [stemmer.stem(x) for x in word1]  #Stemming\n",
    "    spam_list.append((bag_of_words_features_filtered(words), \"spam\"))\n",
    "spam_list[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'nah': True,\n",
       "   \"n't\": True,\n",
       "   'think': True,\n",
       "   'goe': True,\n",
       "   'usf': True,\n",
       "   'live': True,\n",
       "   'around': True,\n",
       "   'though': True},\n",
       "  'ham'),\n",
       " ({'even': True,\n",
       "   'brother': True,\n",
       "   'like': True,\n",
       "   'speak': True,\n",
       "   'treat': True,\n",
       "   'aid': True,\n",
       "   'patent': True},\n",
       "  'ham')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating a list of Anti-spam(ham) emails after data cleaning, word segregation performed on each email one-by-one\n",
    "\n",
    "ham_list=[]\n",
    "for j in range(dfham.shape[0]):\n",
    "    word = word_tokenize(dfham.iloc[j])  #Tokenizing\n",
    "    word1 = [word for word in word if len(word)>1] #  Removing single char words\n",
    "    words = [stemmer.stem(x) for x in word1]  #Stemming\n",
    "    ham_list.append((bag_of_words_features_filtered(words), \"ham\"))\n",
    "ham_list[3:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#80/20 split for training data/test data\n",
    "\n",
    "split1 = 3860\n",
    "split2 = 597\n",
    "\n",
    "# Fit training split to model\n",
    "sentiment_classifier = NaiveBayesClassifier.train(ham_list[:split1]+spam_list[:split2]) # Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.68252187570114\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy, using the training data\n",
    "\n",
    "accuracy_traindata = nltk.classify.util.accuracy(sentiment_classifier,ham_list[:split1]+spam_list[:split2])*100\n",
    "print(accuracy_traindata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.34977578475336\n"
     ]
    }
   ],
   "source": [
    "# Find the accuracy, using the test data\n",
    "accuracy_testdata = nltk.classify.util.accuracy(sentiment_classifier,ham_list[split1:]+spam_list[split2:])*100\n",
    "print(accuracy_testdata)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
